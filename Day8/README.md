## 误差反向传播法

### 简单层实现
- 生成必要的层，以合适的顺序调用正向传播的 forward() 方法。
- 用与正向传播相反的顺序调用反向传播的 backward() 方法，就可以求出想要的导数。
#### 乘法层
- 有两个共通的接口，forward()/backward() 正向传播、反向传播
- 调用 backward() 的顺序与调用 forward() 的顺序相反。
- backward() 的参数中需要输入“关于正向传播时的输出变量的导数”。
#### 加法层
- 不需要特意进行初始化
- forward() 接收 x 和 y 两个参数，将它们相加后输出。
- backward() 将上游传来的导数（dout）原封不动地传递给下游

### 激活层实现
#### ReLU层
- 如果正向传播时的输入 x 大于 0，则反向传播会将上游的值原封不动地传给下游。
- 如果正向传播时的 x 小于等于 0，则反向传播中传给下游的信号将停在此处。
#### Sigmoid 层



### Affine/Softmax 层实现

### 整体实现

- 神经网络学习的目的就是通过调整权重参数，使神经网络的输出（Softmax 的输出）接近教师标签
